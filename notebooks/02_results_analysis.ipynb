{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis: Model Performance Comparison\n",
    "\n",
    "This notebook analyzes and compares the performance of different genomic foundation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_results(results_dir='../results'):\n",
    "    \"\"\"Load all JSON result files.\"\"\"\n",
    "    results = []\n",
    "    for path in Path(results_dir).glob('*.json'):\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "            data['filename'] = path.name\n",
    "            results.append(data)\n",
    "    return results\n",
    "\n",
    "results = load_all_results()\n",
    "print(f\"Loaded {len(results)} result files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics for comparison\n",
    "comparison_data = []\n",
    "\n",
    "for result in results:\n",
    "    if 'overall' in result:\n",
    "        row = {\n",
    "            'model': result.get('model', 'unknown'),\n",
    "            'method': result.get('method', 'unknown'),\n",
    "            'negative_set': result.get('negative_set', 'unknown'),\n",
    "        }\n",
    "        \n",
    "        # Handle both direct values and mean/std dicts\n",
    "        overall = result['overall']\n",
    "        for metric in ['auroc', 'auprc', 'mcc', 'f1']:\n",
    "            if metric in overall:\n",
    "                val = overall[metric]\n",
    "                if isinstance(val, dict):\n",
    "                    row[metric] = val.get('mean', np.nan)\n",
    "                    row[f'{metric}_std'] = val.get('std', np.nan)\n",
    "                else:\n",
    "                    row[metric] = val\n",
    "                    row[f'{metric}_std'] = np.nan\n",
    "        \n",
    "        comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUROC comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Zero-shot performance\n",
    "zs_data = comparison_df[comparison_df['method'] == 'zero_shot']\n",
    "if len(zs_data) > 0:\n",
    "    pivot = zs_data.pivot(index='model', columns='negative_set', values='auroc')\n",
    "    pivot.plot(kind='barh', ax=axes[0])\n",
    "    axes[0].set_title('Zero-Shot AUROC by Model and Negative Set')\n",
    "    axes[0].set_xlabel('AUROC')\n",
    "    axes[0].set_ylabel('Model')\n",
    "    axes[0].axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "    axes[0].legend()\n",
    "\n",
    "# Method comparison (for one negative set)\n",
    "n3_data = comparison_df[comparison_df['negative_set'] == 'N3']\n",
    "if len(n3_data) > 0:\n",
    "    pivot = n3_data.pivot(index='model', columns='method', values='auroc')\n",
    "    pivot.plot(kind='barh', ax=axes[1])\n",
    "    axes[1].set_title('Performance by Method (Negative Set: N3)')\n",
    "    axes[1].set_xlabel('AUROC')\n",
    "    axes[1].set_ylabel('Model')\n",
    "    axes[1].axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "    axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Region Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract per-region metrics\n",
    "region_data = []\n",
    "\n",
    "for result in results:\n",
    "    if 'per_region' in result and result.get('method') == 'zero_shot':\n",
    "        model = result.get('model', 'unknown')\n",
    "        neg_set = result.get('negative_set', 'unknown')\n",
    "        \n",
    "        for region, metrics in result['per_region'].items():\n",
    "            region_data.append({\n",
    "                'model': model,\n",
    "                'negative_set': neg_set,\n",
    "                'region': region,\n",
    "                'auroc': metrics.get('auroc', np.nan),\n",
    "                'auprc': metrics.get('auprc', np.nan),\n",
    "            })\n",
    "\n",
    "region_df = pd.DataFrame(region_data)\n",
    "region_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per-region performance\n",
    "if len(region_df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Filter for one negative set\n",
    "    plot_data = region_df[region_df['negative_set'] == 'N3']\n",
    "    \n",
    "    sns.barplot(data=plot_data, x='region', y='auroc', hue='model', ax=ax)\n",
    "    ax.set_title('Per-Region AUROC (Negative Set: N3)')\n",
    "    ax.set_xlabel('Non-coding Region')\n",
    "    ax.set_ylabel('AUROC')\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load score files for ROC curves\n",
    "def load_scores(results_dir='../results', pattern='scores_*.parquet'):\n",
    "    \"\"\"Load all score parquet files.\"\"\"\n",
    "    scores = {}\n",
    "    for path in Path(results_dir).glob(pattern):\n",
    "        df = pd.read_parquet(path)\n",
    "        key = path.stem.replace('scores_', '')\n",
    "        scores[key] = df\n",
    "    return scores\n",
    "\n",
    "scores = load_scores()\n",
    "print(f\"Loaded {len(scores)} score files\")\n",
    "print(f\"Keys: {list(scores.keys())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(scores)))\n",
    "\n",
    "for (key, df), color in zip(scores.items(), colors):\n",
    "    if 'label' in df.columns and 'score' in df.columns:\n",
    "        fpr, tpr, _ = roc_curve(df['label'], df['score'])\n",
    "        auc = roc_auc_score(df['label'], df['score'])\n",
    "        \n",
    "        ax.plot(fpr, tpr, label=f'{key} (AUC={auc:.3f})', \n",
    "                color=color, linewidth=1.5)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves: Model Comparison')\n",
    "ax.legend(loc='lower right', fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary = comparison_df.groupby(['model', 'method', 'negative_set'])['auroc'].agg(['mean', 'count'])\n",
    "summary = summary.reset_index()\n",
    "summary = summary.sort_values('mean', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE SUMMARY (AUROC)\")\n",
    "print(\"=\"*80)\n",
    "print(summary.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model per negative set\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODEL PER NEGATIVE SET (Zero-Shot)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "zs_results = comparison_df[comparison_df['method'] == 'zero_shot']\n",
    "for neg_set in zs_results['negative_set'].unique():\n",
    "    subset = zs_results[zs_results['negative_set'] == neg_set]\n",
    "    best = subset.loc[subset['auroc'].idxmax()]\n",
    "    print(f\"\\n{neg_set}:\")\n",
    "    print(f\"  Model: {best['model']}\")\n",
    "    print(f\"  AUROC: {best['auroc']:.4f}\")\n",
    "    if not np.isnan(best.get('auroc_std', np.nan)):\n",
    "        print(f\"  Std:   {best['auroc_std']:.4f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
